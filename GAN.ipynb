{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbQOQcXBiElu"
      },
      "source": [
        "check:\n",
        "\n",
        "https://becominghuman.ai/generative-adversarial-networks-for-text-generation-part-3-non-rl-methods-70d1be02350b\n",
        "\n",
        "https://arxiv.org/pdf/1810.06640.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Hg8351tBTl2I"
      },
      "outputs": [],
      "source": [
        "#!pip install \"tensorflow==2.8.*\"\n",
        "!pip install fasttext &> /dev/null\n",
        "!pip install \"tensorflow-text==2.8.*\" &> /dev/null\n",
        "#!pip uninstall -y nltk\n",
        "!pip install -U nltk &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "pxWyRqgcbiIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea348ab9-9a49-4b6b-d3a0-bf7159cd1587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    \n",
        "if COLAB:\n",
        "    drive.mount('/content/drive')\n",
        "    PATH = '/content/drive/MyDrive/NLP_3' #'/content/drive/MyDrive/UniBO/NLP'\n",
        "else:\n",
        "    PATH = './'\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "YckI61SvcE44"
      },
      "outputs": [],
      "source": [
        "#!ls drive/MyDrive/NLP_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "7mARYTEcqQlB"
      },
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "import inspect\n",
        "\n",
        "class Resource_handler:\n",
        "\tdef __init__(self, path_='.'):\n",
        "\t\tself.path = path_\n",
        "\n",
        "\tdef save_resource(self, what):\n",
        "\t\tfrom pickle import dump\n",
        "\t\twith open(join(self.path, what), 'wb') as dfile:\n",
        "\t\t\tprev_frame = inspect.stack()[1].frame\n",
        "\t\t\treturn dump(eval(what, prev_frame.f_globals, prev_frame.f_locals), dfile)\n",
        "\n",
        "\tdef load_resource(self, from_, alt_action=None, args_tuple=(), args_dict=dict()):\n",
        "\t\tfrom pickle import load\n",
        "\t\ttry:\n",
        "\t\t\twith open(join(self.path, from_), 'rb') as dfile:\n",
        "\t\t\t\treturn load(dfile)\n",
        "\t\texcept FileNotFoundError:\n",
        "\t\t\treturn alt_action(*args_tuple, **args_dict) if alt_action else None\n",
        "\n",
        "RH = Resource_handler(PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "tdzyGhSPUGiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b531bd25-8511-4ec2-82b1-3f6de5ecf55c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tf version: 2.8.2\n"
          ]
        }
      ],
      "source": [
        "import io,re\n",
        "import tensorflow as tf\n",
        "print(f'Using tf version: {tf.version.VERSION}')\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "try:\n",
        "    from keras.preprocessing.sequence import pad_sequences\n",
        "except ImportError:\n",
        "    from keras.utils import pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from collections import OrderedDict\n",
        "from nltk.tokenize import word_tokenize\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "import tensorflow_text as tf_text\n",
        "from os.path import join, dirname\n",
        "\n",
        "\n",
        "def clean_data(path = ''):\n",
        "\tDS1 = join(path, 'data/PACCSS-IT.txt')\n",
        "\tDS2 = join(path, 'data/simpitiki-v2.xml')\n",
        "\tCOLS = ['sentence_1', 'sentence_2']\n",
        "\n",
        "\tCSV_PARR_1 = {\n",
        "\t\t'on_bad_lines':'warn',\n",
        "\t\t'delimiter':'\\t',\n",
        "\t\t'engine':'python',\n",
        "\t\t'quotechar':'§',\n",
        "\t\t'skipinitialspace':True,\n",
        "\t\t'decimal':'.'\n",
        "\t}\n",
        "\n",
        "\n",
        "\tdef to_inmem_file(txt):\n",
        "\t\tf = io.StringIO()\n",
        "\t\tf.write(txt)\n",
        "\t\tf.seek(0)\n",
        "\t\treturn f\n",
        "\t\t\n",
        "\n",
        "\tdef clean_1(txt):\n",
        "\t\t# (!!!) Performed on raw data!\n",
        "\t\ttxt = txt\\\n",
        "\t\t\t.replace('\"','')\t\\\n",
        "\t\t\t.replace(\"'\",'')\t\\\n",
        "\t\t\t.replace(\"- \",'')\t\\\n",
        "\t\t\t.replace(\"  \",' ')\t\\\n",
        "\t\t\t.lower()\n",
        "\t\t#txt = re.sub('\\d+[\\./-]\\d+[\\./-]\\d+', '<date>', txt)\n",
        "\t\treturn txt\n",
        "\n",
        "\n",
        "\tdef keep_inbetween(txt, tag):\n",
        "\t\treturn re.sub(f'.*(<{tag}[^<>]*>.+</{tag}>).*', r'\\1', txt, 0, re.DOTALL)\n",
        "\t\t\n",
        "\n",
        "\tdef remove_tags(txt):\n",
        "\t\t'Remove tags; enclosed information is preserved.'\n",
        "\t\treturn re.sub(f'</?[^<>]+>', lambda _:'', txt) if txt!=None else None\n",
        "\t\t\n",
        "\n",
        "\t# ----------- loading data -----------\n",
        "\t#print(pd.read_csv(DS1, **CSV_PARR_1).head(30))\n",
        "\twith open(DS1, 'r') as imf:\n",
        "\t\ttxt = imf.read()\n",
        "\n",
        "\tinmem_DS1 = to_inmem_file(clean_1(txt))\n",
        "\n",
        "\t# ----------- loading data -----------\n",
        "\twith open(DS2, 'r') as imf2:\n",
        "\t\ttxt = imf2.read()\n",
        "\n",
        "\t# ----------- creating DFs -----------\n",
        "\tdf1 = pd.read_csv(inmem_DS1, usecols=COLS, **CSV_PARR_1)\n",
        "\tprint(f'DF 1 has shape {df1.shape}')\n",
        "\n",
        "\t#legend = keep_inbetween(txt, 'types')\n",
        "\t#df_leg = pd.read_xml(to_inmem_file(legend), parser='etree')\n",
        "\n",
        "\tnew_txt = keep_inbetween(txt, 'simplifications')\n",
        "\tdf2 = pd.read_xml(to_inmem_file(new_txt), parser='etree')\n",
        "\n",
        "\tdf2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
        "\tdf2.columns = COLS\n",
        "\tdf2 = df2.applymap(remove_tags)\n",
        "\tprint(f'DF 2 has shape {df2.shape}')\n",
        "\n",
        "\tcombo = pd.concat([df1, df2], copy=False)\n",
        "\n",
        "\treturn combo\n",
        "    \n",
        "\n",
        "def plot_trend(data_x, data_y, x_lab='x', y_lab='y', title='', lines=True, regress=True, figsize=(15,6), line_style='-o'):\n",
        "\tfrom scipy.stats import linregress\n",
        "\timport matplotlib.pyplot as plt\n",
        "\tplt.figure(figsize=figsize)\n",
        "\tplt.margins(0)\n",
        "\tplt.plot(data_x, data_y, line_style, linewidth=1, markersize=10)\n",
        "\tmin_ , max_ = plt.ylim()\n",
        "\trg = max_ - min_\n",
        "\tplt.xlabel(x_lab, fontsize=18)\n",
        "\tplt.xticks(data_x if len(data_x)<figsize[0] else [data_x[0]]+[_ for i,_ in enumerate(data_x) if i%int(len(data_x)/figsize[0])==0]+[data_x[-1]])\n",
        "\tif lines:\n",
        "\t\tfor i,_ in enumerate(data_x): plt.axvline(_, color='gray', linestyle=\":\", ymax=(data_y[i]-min_)/rg)\n",
        "\tif regress:\n",
        "\t\tlr_res = linregress(data_x, data_y)\n",
        "\t\ty = lr_res.intercept + lr_res.slope*data_x\n",
        "\t\tplt.plot([data_x[0], data_x[-1]] , [y[0], y[-1]], 'r')\n",
        "\t\tplt.plot([0, data_x[-1]], [y[-1], y[-1]], color='gray', linestyle='--')\n",
        "\tplt.ylabel(y_lab, fontsize=18)\n",
        "\tif title: plt.title(title, fontsize=18)\n",
        "\tplt.show()\n",
        "\n",
        "def memoization(limit):\n",
        "\tMEM = dict()\n",
        "\t# decorator:\n",
        "\tdef memoizza__(f):\n",
        "\t\tdef g(*x):\n",
        "\t\t\tnonlocal MEM, limit\n",
        "\t\t\tif x in MEM: return MEM[x]\n",
        "\t\t\telse:\n",
        "\t\t\t\tres = f(*x)\n",
        "\t\t\t\tif len(MEM)>=limit and limit!=0:\n",
        "\t\t\t\t\tMEM.popitem() # bad for top-down procedures!\n",
        "\t\t\t\tMEM[x] = res\n",
        "\t\t\t\t# print(MEM)\n",
        "\t\t\t\treturn res\n",
        "\t\treturn g\n",
        "\treturn memoizza__\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "@memoization(0)\n",
        "def word_tk_mem(arg):\n",
        "    return word_tokenize(arg)\n",
        "\n",
        "use_builtins = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Zpn9Z-QFOcH_"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(train_df, test_df, tokenizer):\n",
        "    \"\"\"\n",
        "    Given train corpus and test corpus, builds the corresponding word vocabulary.\n",
        "\n",
        "    --------------\n",
        "    Return: \n",
        "      - word vocabulary: vocabulary index to word\n",
        "      - inverse word vocabulary: word to vocabulary index\n",
        "      - word listing: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "    corpus = list(pd.concat([\n",
        "        train_df['sentence_1'],\n",
        "        train_df['sentence_2'],\n",
        "        test_df['sentence_1'],\n",
        "        test_df['sentence_2']\n",
        "    ]))\n",
        "    word_to_idx = dict()  \n",
        "    idx_to_word = dict()\n",
        "    word_listing = set()\n",
        "    # Get all unique words in corpus \n",
        "    for sentence in corpus:\n",
        "        word_listing |= set(tokenizer(sentence))\n",
        "   # Cast to list\n",
        "    word_listing = sorted(list(word_listing))\n",
        "    # Build vocabulary index to word <idx : word>\n",
        "    idx_to_word = dict(enumerate(word_listing, start=1))\n",
        "    # Build word_to_idx <word : idx>\n",
        "    word_to_idx = dict({_:k for k,_ in idx_to_word.items()})\n",
        "\n",
        "    return idx_to_word, word_to_idx, word_listing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "zJqC-GRdUI-K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "45d23191-ddc6-4b10-a5f4-d7d51bdef269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DF 1 has shape (63006, 2)\n",
            "DF 2 has shape (1166, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          sentence_1  \\\n",
              "0                      ma questo a cosa servirebbe ?   \n",
              "1  salve, avrei bisogno di una informazione piutt...   \n",
              "2      ciao a tutti avrei bisogno di un consiglio .    \n",
              "3  possibilmente uno che avesse bisogno dell aiuto .   \n",
              "4                  questa sarebbe una cosa positiva.   \n",
              "\n",
              "                                 sentence_2  \n",
              "0              a che servono queste cose ?   \n",
              "1  ho bisogno di una informazione urgente .  \n",
              "2          ho bisogno di un suo consiglio .  \n",
              "3          ho bisogno di un vostro aiuto .   \n",
              "4              questa era una nuova cosa .   "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7961684f-2b8e-4d7e-932f-95c411040131\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_1</th>\n",
              "      <th>sentence_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ma questo a cosa servirebbe ?</td>\n",
              "      <td>a che servono queste cose ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>salve, avrei bisogno di una informazione piutt...</td>\n",
              "      <td>ho bisogno di una informazione urgente .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ciao a tutti avrei bisogno di un consiglio .</td>\n",
              "      <td>ho bisogno di un suo consiglio .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>possibilmente uno che avesse bisogno dell aiuto .</td>\n",
              "      <td>ho bisogno di un vostro aiuto .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>questa sarebbe una cosa positiva.</td>\n",
              "      <td>questa era una nuova cosa .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7961684f-2b8e-4d7e-932f-95c411040131')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7961684f-2b8e-4d7e-932f-95c411040131 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7961684f-2b8e-4d7e-932f-95c411040131');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "df = clean_data(PATH)\n",
        "df.dropna(inplace=True)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "p9OSP4dFdEoz"
      },
      "outputs": [],
      "source": [
        "TRAIN_DF, TEST_DF = train_test_split(df, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "bSOsBKWlehv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0123c6b0-7ce3-4d43-985d-36d7cb5a1bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17976\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "idx_to_word, word_to_idx, word_listing = build_vocabulary(TRAIN_DF, TEST_DF, word_tk_mem)\n",
        "VOCAB_SIZE = len(word_listing)\n",
        "print(VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "fBw9i5YxeirL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeeda530-d160-4684-a875-40072b47a0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ],
      "source": [
        "EMBEDD_DIM = 200\n",
        "\n",
        "EMBEDD_FNAME = f'cc.it.{EMBEDD_DIM}.bin'  #ex. 'cc.it.100.bin'\n",
        "\n",
        "try: EMBEDDING_MODEL\n",
        "except NameError:\n",
        "    # Downloading FastText italian pre-trained model\n",
        "    fasttext.FastText.eprint = lambda x: None\n",
        "    try:\n",
        "        print('Loading fasttext model...', flush=True)\n",
        "        EMBEDDING_MODEL = fasttext.load_model(join(PATH, EMBEDD_FNAME))\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "        fasttext.util.download_model('it', if_exists='ignore')  \n",
        "        EMBEDDING_MODEL = fasttext.load_model(join(PATH, EMBEDD_FNAME))\n",
        "\n",
        "print(EMBEDDING_MODEL.get_dimension())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "EZDoz81wha95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9898a8a8-25ba-47c4-e4ed-5a96099e7c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total OOV terms: 1274 (7.087%)\n"
          ]
        }
      ],
      "source": [
        "def check_OOV_terms(embedding_model, vocabulary_terms):\n",
        "  '''Returns a list of out-of-vocabulary (OOV) terms and the corresp. len'''\n",
        "  oov = set(vocabulary_terms).difference(set(embedding_model.words))\n",
        "  return list(oov), len(oov)\n",
        "\n",
        "_, n_oov_terms = check_OOV_terms(EMBEDDING_MODEL, word_listing)\n",
        "print(f\"Total OOV terms: {n_oov_terms} ({n_oov_terms*100/len(word_listing):.03f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "K3x_haRnjgjZ"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = EMBEDDING_MODEL.get_dimension()\n",
        "LOAD_EM = False\n",
        "\n",
        "if LOAD_EM:\n",
        "  EMBEDDING_MATRIX = RH.load_resource('EMBEDDING_MATRIX')\n",
        "else:\n",
        "  def compute_embedding_matrix(embedding_model, idx_to_word):\n",
        "    \"\"\"\n",
        "    Return the embedding matrix of the train vocabulary:\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "    for idx in idx_to_word.keys():\n",
        "      embedding_matrix[idx] = embedding_model.get_word_vector(idx_to_word[idx])\n",
        "    return embedding_matrix\n",
        "\n",
        "  EMBEDDING_MATRIX = compute_embedding_matrix(EMBEDDING_MODEL, idx_to_word)\n",
        "\n",
        "  RH.save_resource('EMBEDDING_MATRIX')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "oxcchSqgjjKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6dc985b-cd13-4303-9d5b-f3e305de6334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights matrix size: (17977, 200)\n",
            "Vocab size: 17976\n"
          ]
        }
      ],
      "source": [
        "print(f'Weights matrix size: {EMBEDDING_MATRIX.shape}')\n",
        "print(f'Vocab size: {VOCAB_SIZE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "g--pPOT0dk4S"
      },
      "outputs": [],
      "source": [
        "def word_to_idx_conversion(df, word_to_idx,len_sentence=None): \n",
        "  \"\"\"\n",
        "  Return input data (sentences) encoded and padded\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  - corpus (train or test dataframe)\n",
        "\n",
        "  Return\n",
        "  ------\n",
        "  - DataFrame: sentences encoded using the word_to_idx dict and padded\n",
        "  \"\"\"\n",
        "  # Conversion\n",
        "  #df.reset_index(drop=True, inplace=True)\n",
        "  converted_df = df.applymap(lambda _: [word_to_idx[token] for token in word_tk_mem(_)])\n",
        "  # Padding\n",
        "  padded_df = converted_df.apply(lambda _: list(pad_sequences(_, padding=\"post\",maxlen=len_sentence)), axis='index')\n",
        "  return padded_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "LZP1P7tqjmW8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "c473cc39-4248-40fa-e21d-9f53c7022e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44918, 2) (19251, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              sentence_1  \\\n",
              "50227  [5823, 13, 11306, 11049, 6885, 12168, 15934, 6...   \n",
              "43336  [11049, 11330, 5676, 7635, 6986, 15934, 3810, ...   \n",
              "12067  [11306, 14020, 17953, 17304, 13630, 7311, 8071...   \n",
              "\n",
              "                                              sentence_2  \n",
              "50227  [11049, 6885, 12168, 15934, 13781, 6950, 15522...  \n",
              "43336  [11049, 11330, 5676, 7635, 6986, 15934, 3810, ...  \n",
              "12067  [14020, 17953, 17304, 13630, 8404, 19, 0, 0, 0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1bbf264b-9f0e-4a06-8607-b67410d82553\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_1</th>\n",
              "      <th>sentence_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50227</th>\n",
              "      <td>[5823, 13, 11306, 11049, 6885, 12168, 15934, 6...</td>\n",
              "      <td>[11049, 6885, 12168, 15934, 13781, 6950, 15522...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43336</th>\n",
              "      <td>[11049, 11330, 5676, 7635, 6986, 15934, 3810, ...</td>\n",
              "      <td>[11049, 11330, 5676, 7635, 6986, 15934, 3810, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12067</th>\n",
              "      <td>[11306, 14020, 17953, 17304, 13630, 7311, 8071...</td>\n",
              "      <td>[14020, 17953, 17304, 13630, 8404, 19, 0, 0, 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bbf264b-9f0e-4a06-8607-b67410d82553')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1bbf264b-9f0e-4a06-8607-b67410d82553 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1bbf264b-9f0e-4a06-8607-b67410d82553');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "ENCODED_TRAIN_DF = word_to_idx_conversion(TRAIN_DF, word_to_idx)\n",
        "m_len_input=max([len(i) for i in ENCODED_TRAIN_DF['sentence_1']])#in teoria è fissa e uguale per tutti ma come al solito meglio essere sicuri\n",
        "ENCODED_TEST_DF = word_to_idx_conversion(TEST_DF, word_to_idx,len_sentence=m_len_input)\n",
        "print(ENCODED_TRAIN_DF.shape, ENCODED_TEST_DF.shape)\n",
        "ENCODED_TRAIN_DF.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "qPuScbz3jsrk"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATASET = ENCODED_TRAIN_DF\n",
        "#EMBEDDING_MATRIX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LChDvvcijWe"
      },
      "source": [
        "# Autoencoder (AE)\n",
        "\n",
        "\"we first train an auto-encoder on a large corpus of real sentences. Then, while training the GAN, to get “real” samples we input real sentences to the encoder of the auto-encoder and get the corresponding sentence vectors.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "WRPEZkpLm2ug"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model,Sequential\n",
        "Adam = tf.optimizers.Adam\n",
        "from keras.layers import Dense, Dropout, Input, LeakyReLU, Rescaling, LSTM, RepeatVector\n",
        "from keras import backend as K\n",
        "#import tensorflow as tf\n",
        "#from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from auxiliary import no_out\n",
        "\n",
        "import io, sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "svwmdBsQkTsq"
      },
      "outputs": [],
      "source": [
        "MAX_SENTENCE_LEN = 20 #TRAIN_DATASET.iloc[0][0].shape[0]; MAX_SENTENCE_LEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "yINrtkZLiivC"
      },
      "outputs": [],
      "source": [
        "def create_AE(timesteps, embedd_dim, latent_dim, vocab_size):\n",
        "  '''Creates an autoencoder for sequences using LSTM'''\n",
        "  global EMBEDDING_MATRIX\n",
        "  inputs = Input(shape=(timesteps, )) # ,embedd_dim\n",
        "\n",
        "  embed_layer = tf.keras.layers.Embedding(\n",
        "    vocab_size,   #  in dim\n",
        "    embedd_dim,     #  out dim\n",
        "    embeddings_initializer = tf.keras.initializers.Constant(EMBEDDING_MATRIX),\n",
        "    mask_zero = True,\n",
        "    trainable = False\n",
        "  )(inputs)\n",
        "\n",
        "  encoded = LSTM(latent_dim, return_sequences=True)(embed_layer)  #dropout=.2,  \n",
        "  \n",
        "  decoded = LSTM(latent_dim, return_sequences=True)(encoded) #  # embedd_dim\n",
        "  decoded = Dense(vocab_size, activation=tf.keras.activations.softmax)(decoded)\n",
        "\n",
        "  autoencoder = Model(inputs, decoded)\n",
        "  encoder = Model(inputs, encoded)\n",
        "  #decoder = Model(Input(shape=(timesteps, latent_dim)), decoded)\n",
        "\n",
        "  return autoencoder, encoder, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "ftfs7FOrpsxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14dc0812-7220-4f1d-8e0c-4072fbf78351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_12 (InputLayer)       [(None, 20)]              0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 20, 200)           3595400   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 20, 50)            50200     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 20, 50)            20200     \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 20, 17977)         916827    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,582,627\n",
            "Trainable params: 987,227\n",
            "Non-trainable params: 3,595,400\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "INNER_DIM = 50\n",
        "\n",
        "AE, ENC, _ = create_AE(MAX_SENTENCE_LEN, EMBEDD_DIM, INNER_DIM, VOCAB_SIZE+1)  # EMBEDD_DIM//MAX_SENTENCE_LEN\n",
        "AE.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "A0-L-j4wpvDa"
      },
      "outputs": [],
      "source": [
        "adam_pp = {\n",
        "\t'learning_rate': .002,\t# def. 0.001\n",
        "\t'beta_1': .9,\t\t\t# def. 0.9\n",
        "\t'beta_2': .999,\t\t\t# def. 0.999\n",
        "\t'epsilon': 1e-12\n",
        "}\n",
        "\n",
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.mean_squared_error\n",
        "    self.reduction = tf.compat.v1.losses.Reduction.NONE #losses_utils.ReductionV2.SUM\n",
        "\n",
        "  def __call__(self, y_true, y_pred, sample_weight):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    # Mask off the losses on padding.\n",
        "    #mask = np.apply_along_axis(lambda _: np.allclose(_, np.zeros(_.shape)), 2, y_true)\n",
        "    mask = np.asarray(y_true).any(axis=2)\n",
        "    #print(mask.shape)\n",
        "    loss = tf.math.multiply(loss, mask)\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)\n",
        "\n",
        "AE.compile(optimizer=Adam(**adam_pp), loss=MaskedLoss()) #mse ?\n",
        "\n",
        "def iterate(ds, batch_s=16, silent=True):\n",
        "  global VOCAB_SIZE, MAX_SENTENCE_LEN # to fact\n",
        "  while True:\n",
        "    #gc.collect()\n",
        "    for i in range(0, len(ds)-batch_s+1, batch_s):\n",
        "      l = []\n",
        "      one_h = np.zeros((batch_s, MAX_SENTENCE_LEN, VOCAB_SIZE+1), dtype='float32')\n",
        "      for k in range(batch_s):\n",
        "        l.append(ds.iloc[i+k][:MAX_SENTENCE_LEN])  # TODO REMOVE LAST TR.\n",
        "        for word in range(MAX_SENTENCE_LEN):\n",
        "          one_h[k, word, ds.iloc[i+k][word]] = .9\n",
        "      el = tf.constant(l)\n",
        "      #print(el.shape, one_h.shape)\n",
        "      yield el, one_h\n",
        "      #yield tf.convert_to_tensor(ds.iloc[i:i+batch_s].to_numpy()) # bug?\n",
        "    if not silent: print('\\nRESTARTING ITERATION')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "EQqnhW9qAF1D"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect();\n",
        "#AE.load_weights('AE2')\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "WHOLE_DS = pd.concat([\n",
        "  TRAIN_DATASET['sentence_1'],\n",
        "  TRAIN_DATASET['sentence_2']],\n",
        "  ignore_index=True\n",
        ")\n",
        "\n",
        "sentence_ds_itr = iterate(WHOLE_DS, batch_s=BATCH_SIZE, silent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "aeK6X1rs_D3D"
      },
      "outputs": [],
      "source": [
        "LOAD_W = True\n",
        "\n",
        "if LOAD_W:\n",
        "  !cp drive/MyDrive/NLP_3/AE3* .\n",
        "  AE.load_weights('AE3')\n",
        "\n",
        "N = len(WHOLE_DS)//BATCH_SIZE  # //10: one ds iteration every 10 epochs\n",
        "losses = []\n",
        "\n",
        "try:\n",
        "  for epoch in range(0):\n",
        "    ts0 = tf.timestamp()\n",
        "    tot_loss = 0.\n",
        "    for batch in range(N):\n",
        "      el, one_hot = next(sentence_ds_itr)\n",
        "      tot_loss += AE.train_on_batch(el, one_hot, return_dict=True)['loss']\n",
        "    print(end=f'epoch {epoch} loss: {tot_loss*1000/(N*BATCH_SIZE):.4f} e-3  (epoch took {tf.timestamp()-ts0.numpy():.1f})')\n",
        "    if epoch>=3:\n",
        "      AE.save_weights('AE3')\n",
        "      !cp -u AE3* drive/MyDrive/NLP_3/\n",
        "      print(end = '  -- weights saved')\n",
        "      losses.append(tot_loss)\n",
        "      gc.collect()\n",
        "    print()\n",
        "except KeyboardInterrupt:\n",
        "  plot_trend(range(len(losses)), losses)\n",
        "  print('\\nMANUAL STOP')\n",
        "\n",
        "# epoch 27 loss: 0.0739 e-3  (epoch took 244.2)\n",
        "# epoch +12 loss: 0.0305 e-3  (epoch took 243.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator2:\n",
        "\tdef __init__(self, word_to_idx, idx_to_word, tokenizer, fun=lambda _:_):\n",
        "\t\tself.w2i = word_to_idx\n",
        "\t\tself.i2w = idx_to_word\n",
        "\t\tself.tk = tokenizer\n",
        "\t\tself.f = fun\t\n",
        "\n",
        "\tdef tr(self, what):\n",
        "\t\tif isinstance(what, str):\n",
        "\t\t\ttr = [self.w2i[token] for token in self.tk(what)]\n",
        "\t\t\treturn self.f(tr) if flat else self.f([tr])\n",
        "\t\telse:\n",
        "\t\t\ttr = [ ' '.join([self.i2w[token] for token in s]) for s in what]\n",
        "\t\t\treturn '\\n'.join(tr)"
      ],
      "metadata": {
        "id": "tX6PUgHIBcFm"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "YXmLJyF_WC_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecce6174-1bc4-45a2-ec71-03cf8700c3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ho constatato che davide ha una brutta voce . . . . . . . . . . . .\n",
            "ho constatato che davide ha una brutta avevano . . . . . . . . . . . .\n",
            "ho constatato che davide ha una avevano brutta . . . . . . . . . . . .\n",
            "ho constatato che una avevano brutta ha davide . . . . . . . . . . . .\n",
            "l.d . . . . . . . . . . . . . . . . . . .\n",
            "avevano di discorsi . . . . . . . . . . . . . . . . .\n",
            "assai nell'ambiente è il cammino . . . . . . . . . . . . . . .\n",
            "più decadenza è il cammino . . . . . . . . . . . . . . .\n",
            "più propriamente è il cammino . . . . . . . . . . . . . . .\n",
            "più propriamente fu il cammino . . . . . . . . . . . . . . .\n",
            "propriamente è il cammino . . . . . . . . . . . . . . . .\n",
            "propriamente . . . . . . . . . . . . . . . . . . .\n",
            "più veloce è il cammino . . . . . . . . . . . . . . .\n",
            "poi si farà . . . . . . . . . . . . . . . . .\n"
          ]
        }
      ],
      "source": [
        "tr = Translator2(word_to_idx,{0:'<ZERO>', **idx_to_word}, word_tokenize) #, fun=tf.convert_to_tensor)\n",
        "\n",
        "def run_model(model_, sentence):\n",
        "  global iterate, word_to_idx_conversion, word_to_idx, MAX_SENTENCE_LEN\n",
        "  sl = sentence if type(sentence) is list else [sentence]\n",
        "  pred_test_itr = iterate(word_to_idx_conversion(\n",
        "      pd.DataFrame(sl),\n",
        "      word_to_idx,\n",
        "      len_sentence=MAX_SENTENCE_LEN).iloc[:,0],\n",
        "    batch_s=len(sl))\n",
        "  in_, one_h = next(pred_test_itr)\n",
        "  out_ = model_.predict(in_)\n",
        "  return np.asarray(tf.math.argmax(out_, axis=-1))\n",
        "\n",
        "print(tr.tr(run_model(AE, [\n",
        "  \"ho constatato che davide ha una brutta voce.\",\n",
        "  \"ho constatato che davide ha una brutta barba.\",\n",
        "  \"ho constatato che davide ha una barba brutta.\",\n",
        "  \"ho constatato che una barba brutta ha davide.\",\n",
        "  \"Giovanni.\",\n",
        "  \"barba di Giovanni.\",\n",
        "  \"assai periglioso è il cammino.\",\n",
        "  \"più periglioso è il cammino.\",\n",
        "  \"più vago è il cammino.\",\n",
        "  \"più vago fu il cammino.\",\n",
        "  \"vago è il cammino.\",\n",
        "  \"vago.\",\n",
        "  \"più veloce è il cammino.\",\n",
        "  \"poi si farà.\",\n",
        "]\n",
        ")))  #.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j09SKYngipNp"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam_pp = {\n",
        "\t'learning_rate': .00002,\t# def. 0.001\n",
        "\t'beta_1': .8,\t\t\t# def. 0.9\n",
        "\t'beta_2': .999,\t\t\t# def. 0.999\n",
        "\t'epsilon': 1e-10\n",
        "}\n",
        "relu_initializer = tf.keras.initializers.HeUniform()\n",
        "#oth_initializer  = tf.keras.initializers.GlorotNormal()"
      ],
      "metadata": {
        "id": "XcTWbhv88jBr"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !!! do NOT RUN this cell to continue training! !!!\n",
        "acc_loss = []\n",
        "nets = None\n",
        "#gc.collect();"
      ],
      "metadata": {
        "id": "tH6jeF5bN18q"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gen(input_shape, dims, id_='0'):\n",
        "    global adam_pp, relu_initializer\n",
        "    \n",
        "    in_ = gen = Input(input_shape)\n",
        "    for d in dims[:-1]:\n",
        "      gen = Dense(units=d, activation=LeakyReLU(0.2), kernel_initializer=relu_initializer)(gen)\n",
        "    gen = Dense(dims[-1], activation=tf.keras.activations.softmax)(gen)\n",
        "\n",
        "    #gen = tf.math.argmax(gen, axis=-1)\n",
        "    gen = Model(in_, gen, name=id_)\n",
        "    gen.compile(loss=tf.keras.losses.BinaryCrossentropy(\n",
        "        reduction=tf.keras.losses.Reduction.SUM\n",
        "    ), optimizer=Adam(**adam_pp))\n",
        "    return gen\n",
        "\n",
        "if not nets:\n",
        "  GEN = create_gen((MAX_SENTENCE_LEN, INNER_DIM), (32, 512, 1024, 128, INNER_DIM), id_='generator')\n",
        "  GEN.summary()"
      ],
      "metadata": {
        "id": "IPenIPYC8-6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b23fcf-a924-4132-9e8b-d8480b505682"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_13 (InputLayer)       [(None, 20, 50)]          0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 20, 32)            1632      \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 20, 512)           16896     \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 20, 1024)          525312    \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 20, 128)           131200    \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 20, 50)            6450      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 681,490\n",
            "Trainable params: 681,490\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dis(input_shape, id_='0'):\n",
        "    global adam_pp, relu_initializer\n",
        "    \n",
        "    in_ = dis = Input(input_shape)\n",
        "    #dis = Dense(units=128, activation=LeakyReLU(0.2), kernel_initializer=relu_initializer)(dis)\n",
        "    #dis = Dropout(0.2)(dis)\n",
        "    \n",
        "    dis = Dense(units=64, activation=LeakyReLU(0.2), kernel_initializer=relu_initializer)(dis)\n",
        "    #dis = Dropout(0.2)(dis)\n",
        "\n",
        "    dis = tf.keras.layers.Flatten()(dis)\n",
        "    \n",
        "    dis = Dense(units=512, activation=LeakyReLU(0.2), kernel_initializer=relu_initializer)(dis)\n",
        "    dis = Dropout(0.2)(dis)\n",
        "\n",
        "    dis = Dense(units=256, activation=LeakyReLU(0.2), kernel_initializer=relu_initializer)(dis)\n",
        "    dis = Dropout(0.3)(dis)\n",
        "\n",
        "    dis = Dense(units=1, activation='sigmoid')(dis)\n",
        "\n",
        "    #gen = tf.math.argmax(gen, axis=-1)\n",
        "    dis = Model(in_, dis, name=id_)\n",
        "    dis.compile(loss=tf.keras.losses.BinaryCrossentropy(\n",
        "        reduction=tf.keras.losses.Reduction.SUM\n",
        "    ), optimizer=Adam(**adam_pp))\n",
        "    return dis\n",
        "\n",
        "if not nets:\n",
        "  DIS = create_dis((MAX_SENTENCE_LEN, INNER_DIM), id_='discr')\n",
        "  DIS.summary()"
      ],
      "metadata": {
        "id": "a5atrvbA9FzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43148d04-3167-40be-f06d-4762f7a2e933"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discr\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_14 (InputLayer)       [(None, 20, 50)]          0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 20, 64)            3264      \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 1280)              0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 512)               655872    \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 790,721\n",
            "Trainable params: 790,721\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gan(discriminator, generator, in_shape):\n",
        "  '''\n",
        "  class MaskedLoss2(tf.keras.losses.Loss):\n",
        "    def __init__(self, masked_value=0.):\n",
        "      self.name = 'masked_loss2'\n",
        "      self.loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "        #from_logits=False,\n",
        "        #reduction='none')\n",
        "      self.mv = masked_value\n",
        "      self.reduction = tf.compat.v1.losses.Reduction.NONE #\n",
        "\n",
        "    def __call__(self, y_true, y_pred, sample_weight):\n",
        "      # Calculate the loss for each item in the batch.\n",
        "      loss = self.loss(y_true, y_pred)\n",
        "      print(loss.shape)\n",
        "      # Mask off the losses on padding.\n",
        "      mask = tf.cast(y_true != self.mv, tf.float32)\n",
        "      loss *= mask\n",
        "      print(tf.reduce_sum(loss).shape)\n",
        "      # Return the total.\n",
        "      return tf.reduce_sum(loss)\n",
        "  ''';\n",
        "\n",
        "  discriminator.trainable = False\n",
        "  gan_input = Input(shape=in_shape)\n",
        "  x = tf.keras.layers.Masking()(gan_input)\n",
        "  x = generator(x)  # gan_input\n",
        "  gan_output = discriminator(x)\n",
        "  gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "  gan.compile(loss=tf.keras.losses.BinaryCrossentropy(\n",
        "    #from_logits=True,\n",
        "    reduction=tf.keras.losses.Reduction.SUM\n",
        "  ), optimizer=Adam(**adam_pp))  # 'binary_crossentropy'\n",
        "  return gan\n",
        "\n",
        "if not nets:\n",
        "  GAN = create_gan(DIS, GEN, (MAX_SENTENCE_LEN, INNER_DIM))\n",
        "  GAN.summary()"
      ],
      "metadata": {
        "id": "8ylOQS6wLM-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0da0eb6-f473-4808-82f5-2c4bfbef03fc"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_15 (InputLayer)       [(None, 20, 50)]          0         \n",
            "                                                                 \n",
            " masking_3 (Masking)         (None, 20, 50)            0         \n",
            "                                                                 \n",
            " generator (Functional)      (None, 20, 50)            681490    \n",
            "                                                                 \n",
            " discr (Functional)          (None, 1)                 790721    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,472,211\n",
            "Trainable params: 681,490\n",
            "Non-trainable params: 790,721\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE REPLACED WITH A HUMAN IMPLEMENTATION\n",
        "class DECc:\n",
        "  def __init__(self, AE):\n",
        "    self.AE = AE\n",
        "    self.dec_1st_layer = [i for i,e in enumerate(AE.layers) if type(e) is tf.keras.layers.LSTM][1]\n",
        "  def __call__(self, x):\n",
        "    #x = tf.zeros((1,20,50))\n",
        "    for l in self.AE.layers[self.dec_1st_layer:]:\n",
        "      x = l(x)\n",
        "    return tf.math.argmax(x, axis=-1)\n",
        "\n",
        "DEC = DECc(AE)"
      ],
      "metadata": {
        "id": "2si0Ey77Yxlv"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ENC( tf.ones((1,20)))"
      ],
      "metadata": {
        "id": "GpNsOl8Ncpie"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RED_FACTOR = 10\n",
        "LOAD_W_GAN = False\n",
        "\n",
        "def training(epochs=1, batch_size=1024):\n",
        "    global acc_loss, nets, ds_iter, DIS, GEN, GAN, MAX_SENTENCE_LEN, INNER_DIM, DEC, ENC, iterate\n",
        "\n",
        "    if LOAD_W_GAN:\n",
        "      !cp drive/MyDrive/NLP_3/GAN* .\n",
        "      AE.load_weights('GAN1')\n",
        "\n",
        "\n",
        "    def iterate_4gan(ds_iterator1, ds_iterator2):\n",
        "      while True:\n",
        "        (complex_, _), (simple, _) = next(ds_iterator1), next(ds_iterator2)\n",
        "        yield ENC(complex_), ENC(simple), (complex_, simple)\n",
        "    #EOFUN\n",
        "\n",
        "    ds_iterator = iterate_4gan(\n",
        "        iterate(TRAIN_DATASET['sentence_1'], batch_s=batch_size),\n",
        "        iterate(TRAIN_DATASET['sentence_2'], batch_s=batch_size)\n",
        "    )\n",
        "\n",
        "    if (not nets):\n",
        "      nets = [GEN, DIS, GAN]\n",
        "      # print(gan.metrics_names)\n",
        "    else:\n",
        "      print('already existent gan, continuing...\\n')\n",
        "    g, d, gan = nets\n",
        "\n",
        "    N = len(TRAIN_DATASET['sentence_1'])//batch_size//RED_FACTOR # N. OF BATCHES RUN, FOR EACH EPOCH\n",
        "    for e in range(1, epochs+1):\n",
        "      ts0 = tf.timestamp()\n",
        "      d_loss = 0.\n",
        "      e_loss = 0.\n",
        "      print(f\" === Epoch {len(acc_loss)+1:2d} ===\")\n",
        "      for batch in range(N):\n",
        "        # STEP 1 -------------------------------------------------\n",
        "        E1, E2, (complex_, _) = next(ds_iterator)\n",
        "        # Generate \"fake\" data from E1\n",
        "        G1 = g.predict(E1) #.astype(np.int32) #no_out\n",
        "        x = np.concatenate([G1, E2]) ###[indices]\n",
        "        #print(len(REAL_BATCH), len( generated_batch),x.shape)\n",
        "\n",
        "        # Labels for generated and real data\n",
        "        y = np.zeros(2*batch_size)\n",
        "        y[0:batch_size] = .9  # label smoothing\n",
        "\n",
        "        #Pre train discriminator on  fake and real data  before starting the gan. \n",
        "        d.trainable = True\n",
        "        d_loss += d.train_on_batch(x, y=y, reset_metrics=True, return_dict=True)['loss'] #no_out\n",
        "        #print(end=f'Discriminator loss: {d_loss}; ')\n",
        "\n",
        "        # STEP 2 -------------------------------------------------\n",
        "        # Treating the noised input of the Generator as real data\n",
        "        y_gen = np.ones(batch_size) #np.ones(batch_size)\n",
        "\n",
        "        # During the training of gan, \n",
        "        # the weights of discriminator should be fixed. \n",
        "        # We can enforce that by setting the trainable flag\n",
        "        d.trainable = False\n",
        "\n",
        "        #training  the GAN by alternating the training of the Discriminator \n",
        "        #and training the chained GAN model with Discriminator’s weights freezed.\n",
        "        e_loss += gan.train_on_batch(next(ds_iterator)[0], y_gen, reset_metrics=True, return_dict=True)['loss']\n",
        "\n",
        "      acc_loss += [(d_loss, e_loss)]\n",
        "      print(f'dis loss is {acc_loss[-1][0]*1000.//(N*batch_size):0.7f} e-3')\n",
        "      print(f'gan loss is {acc_loss[-1][1]*1000.//(N*batch_size):0.7f} e-3')  #*1000/(N*batch_size)\n",
        "      print(f'epoch took {tf.timestamp()-ts0:.1f}s')\n",
        "      if e>2:\n",
        "        print(tr.tr(np.asarray(complex_)[:1]))\n",
        "        print(tr.tr(np.asarray(DEC(G1))[:1]))  # !!!using the last G1\n",
        "        gan.save_weights('GAN1')\n",
        "        !cp -u GAN* drive/MyDrive/NLP_3/\n",
        "\n",
        "      gc.collect()"
      ],
      "metadata": {
        "id": "vIpTYKdlOLRN"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training(500, batch_size=512)"
      ],
      "metadata": {
        "id": "qsJgd5M5iXSH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "backup_GAN1_AE3.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}