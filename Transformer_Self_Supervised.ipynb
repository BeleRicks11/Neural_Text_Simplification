{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "wUTHFEE_w0Wz",
        "outputId": "9720684b-b13e-43a1-8828-bb8a23905484"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from utilities import resize\n",
        "resize(100)\n",
        "\n",
        "#!pip install \"tensorflow==2.8.*\"\n",
        "!pip install fasttext > /dev/null\n",
        "!pip install \"tensorflow-text==2.8.*\" > /dev/null   #TODO CHECK\n",
        "!pip install datasets > /dev/null\n",
        "!pip install sacrebleu > /dev/null\n",
        "!pip install sacremoses > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfFEFZJ6w8BF",
        "outputId": "8b79ea05-7e5e-4d1d-98e1-7fcac81285a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "try:\n",
        "    from keras.preprocessing.sequence import pad_sequences\n",
        "except ImportError:\n",
        "    from keras.utils import pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from collections import OrderedDict\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle \n",
        "import typing\n",
        "import random\n",
        "from datasets import load_metric\n",
        "from typing import Any, Tuple\n",
        "import tensorflow_text as tf_text\n",
        "from os.path import join, dirname\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "\n",
        "from data_cleaning import clean_data\n",
        "from data_preparation import build_vocabulary\n",
        "    \n",
        "from utilities import memoization, plot_trend\n",
        "\n",
        "#tf.config.run_functions_eagerly(True)\n",
        "\n",
        "@memoization(0)\n",
        "def word_tk_mem(arg):\n",
        "    return word_tokenize(arg)\n",
        "\n",
        "nltk.download('punkt')\n",
        "if COLAB:\n",
        "    drive.mount('/content/drive')\n",
        "    PATH = '/content/drive/MyDrive/UniBO/NLP'\n",
        "else:\n",
        "    PATH = './'\n",
        "use_builtins = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "zuzNGavFxIb7",
        "outputId": "bdd173e4-2524-47b4-ac25-09342ca90cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/UniBO/NLP\n",
            "DF 1 has shape (63006, 2)\n",
            "DF 2 has shape (1166, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-418c7db8-8f52-4c36-8284-e8239605ac47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_1</th>\n",
              "      <th>sentence_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ma questo a cosa servirebbe ?</td>\n",
              "      <td>a che servono queste cose ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>salve, avrei bisogno di una informazione piutt...</td>\n",
              "      <td>ho bisogno di una informazione urgente .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ciao a tutti avrei bisogno di un consiglio .</td>\n",
              "      <td>ho bisogno di un suo consiglio .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>possibilmente uno che avesse bisogno dell aiuto .</td>\n",
              "      <td>ho bisogno di un vostro aiuto .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>questa sarebbe una cosa positiva.</td>\n",
              "      <td>questa era una nuova cosa .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>possibilmente uno che avesse bisogno dell aiuto .</td>\n",
              "      <td>ho bisogno di un aiuto .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>avrei bisogno di un aiuto .</td>\n",
              "      <td>ho bisogno di un vostro aiuto .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>quale sarebbe allora la soluzione giusta ?</td>\n",
              "      <td>è questa la soluzione giusta ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>salve , avrei bisogno di un vostro aiuto .</td>\n",
              "      <td>abbiamo bisogno del vostro aiuto .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>salve , avrei bisogno di un vostro aiuto .</td>\n",
              "      <td>ho bisogno del vostro aiuto .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>quali consigli darebbe ai genitori ?</td>\n",
              "      <td>che consiglio vuole dare ai genitori ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>che ne sarebbe della natura della realtà ?</td>\n",
              "      <td>ma di che natura sono queste realtà ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>avrei bisogno di un aiuto .</td>\n",
              "      <td>abbiamo bisogno di un aiuto soprattutto fisico .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ora avrei bisogno di un consiglio .</td>\n",
              "      <td>ho bisogno di un suo consiglio .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>quale potrebbe essere la ragione ?</td>\n",
              "      <td>ma quale poteva essere questa ragione ?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-418c7db8-8f52-4c36-8284-e8239605ac47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-418c7db8-8f52-4c36-8284-e8239605ac47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-418c7db8-8f52-4c36-8284-e8239605ac47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           sentence_1  \\\n",
              "0                       ma questo a cosa servirebbe ?   \n",
              "1   salve, avrei bisogno di una informazione piutt...   \n",
              "2       ciao a tutti avrei bisogno di un consiglio .    \n",
              "3   possibilmente uno che avesse bisogno dell aiuto .   \n",
              "4                   questa sarebbe una cosa positiva.   \n",
              "5   possibilmente uno che avesse bisogno dell aiuto .   \n",
              "6                         avrei bisogno di un aiuto .   \n",
              "7          quale sarebbe allora la soluzione giusta ?   \n",
              "8         salve , avrei bisogno di un vostro aiuto .    \n",
              "9         salve , avrei bisogno di un vostro aiuto .    \n",
              "10               quali consigli darebbe ai genitori ?   \n",
              "11        che ne sarebbe della natura della realtà ?    \n",
              "12                       avrei bisogno di un aiuto .    \n",
              "13               ora avrei bisogno di un consiglio .    \n",
              "14                 quale potrebbe essere la ragione ?   \n",
              "\n",
              "                                          sentence_2  \n",
              "0                       a che servono queste cose ?   \n",
              "1           ho bisogno di una informazione urgente .  \n",
              "2                   ho bisogno di un suo consiglio .  \n",
              "3                   ho bisogno di un vostro aiuto .   \n",
              "4                       questa era una nuova cosa .   \n",
              "5                          ho bisogno di un aiuto .   \n",
              "6                   ho bisogno di un vostro aiuto .   \n",
              "7                    è questa la soluzione giusta ?   \n",
              "8                 abbiamo bisogno del vostro aiuto .  \n",
              "9                      ho bisogno del vostro aiuto .  \n",
              "10           che consiglio vuole dare ai genitori ?   \n",
              "11             ma di che natura sono queste realtà ?  \n",
              "12  abbiamo bisogno di un aiuto soprattutto fisico .  \n",
              "13                  ho bisogno di un suo consiglio .  \n",
              "14          ma quale poteva essere questa ragione ?   "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = clean_data(PATH)\n",
        "df.dropna(inplace=True)\n",
        "df.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/1C-CBzudbdaDrClPwTwqEgQVCP-Af40Za/view?usp=sharing\n",
        "\n",
        "https://drive.google.com/file/d/1azLhm1to1JWiKOLdaZ3NQ2iouKnigVyD/view?usp=sharing"
      ],
      "metadata": {
        "id": "jwRBhZHTcW3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn4D6CS5-WTA"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/paisa_clean_95_th.txt ./paisa_clean_95_th.txt \n",
        "!cp /content/drive/MyDrive/paisa_small.txt ./paisa_small.txt #subset of paisa with only 100000 sentences "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KdutYNBxOQs"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LENGHT=111 #obtained from texting covers 95th percentile of the data in the full paisa dataset \n",
        "TRAIN_DF, TEST_DF = train_test_split(df, test_size=0.3)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def format_dataset(cplx, simpl):\n",
        "  return ({\"encoder_inputs\": cplx, \"decoder_inputs\": simpl[:, :-1],}, simpl[:, 1:])\n",
        "\n",
        "TRAIN_DATASET = tf.data.TextLineDataset(['paisa_clean_95_th.txt']).batch(BATCH_SIZE).shuffle(2048).prefetch(16).cache()\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tpr1_uYwxbSK"
      },
      "outputs": [],
      "source": [
        "\n",
        "VOCABULARY_SIZE=20000+2 #+2 one for unknown one for empty string(padding)\n",
        "\n",
        "def cr_vocab_vector(text_dataset):\n",
        "  def tf_lower_and_split_punct(text): \n",
        "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.join(['[START]',text, '[END]'], separator=' ')\n",
        "    return text\n",
        "  Vec_layer=tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct,max_tokens=VOCABULARY_SIZE,output_sequence_length=MAX_SEQ_LENGHT+2)\n",
        "  Vec_layer.adapt(text_dataset)#vocabulary generation using the largest dataset more statistical significance\n",
        "  return Vec_layer.get_vocabulary(),Vec_layer\n",
        "\n",
        "IDX_TO_WORD,VECTORIZER=cr_vocab_vector(TRAIN_DATASET) \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LESUbr5Z99Bk"
      },
      "outputs": [],
      "source": [
        "WORD_LENGTH=[len(i) for i in IDX_TO_WORD]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btgPKZOQMX8V"
      },
      "outputs": [],
      "source": [
        "#sentence_1=[VECTORIZER(i) for  i in TRAIN_DF['sentence_1']]\n",
        "#sentence_2=[VECTORIZER(i) for  i in TRAIN_DF['sentence_2']] #used for testing no longer needed \n",
        "\n",
        "#TRAIN_DATASET =tf.data.Dataset.from_tensor_slices((sentence_1,sentence_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s4Ddogn_nlTW"
      },
      "outputs": [],
      "source": [
        "sentence_1=[VECTORIZER(i) for  i in TEST_DF['sentence_1']]\n",
        "sentence_2=[VECTORIZER(i) for  i in TEST_DF['sentence_2']]\n",
        "\n",
        "TEST_DATASET =tf.data.Dataset.from_tensor_slices((sentence_1,sentence_2)).batch(BATCH_SIZE).shuffle(2048).prefetch(16).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nX44R5sieluE"
      },
      "outputs": [],
      "source": [
        "#TRAIN_DATASET =TRAIN_DATASET.batch(64).shuffle(2048).prefetch(16).cache() #testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wRFQu_8N4eCq"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATASET = tf.data.TextLineDataset(['/content/paisa_small.txt']).batch(BATCH_SIZE).shuffle(2048).prefetch(16).cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwVqsBbduYIE",
        "outputId": "cc4803ed-6b3f-4fac-a415-f61755c3c856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading fasttext model...\n"
          ]
        }
      ],
      "source": [
        "EMBEDD_DIM = 200\n",
        "\n",
        "EMBEDD_FNAME = f'cc.it.{EMBEDD_DIM}.bin'  #ex. 'cc.it.100.bin'\n",
        "\n",
        "try: EMBEDDING_MODEL\n",
        "except NameError:\n",
        "    # Downloading FastText italian pre-trained model\n",
        "    fasttext.FastText.eprint = lambda x: None\n",
        "    try:\n",
        "        print('Loading fasttext model...', flush=True)\n",
        "        EMBEDDING_MODEL = fasttext.load_model(join(PATH, EMBEDD_FNAME))\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "        fasttext.util.download_model('it', if_exists='ignore')  \n",
        "        EMBEDDING_MODEL = fasttext.load_model(join(PATH, EMBEDD_FNAME))\n",
        "\n",
        "print(EMBEDDING_MODEL.get_dimension())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zFygSRjZthU-"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = EMBEDDING_MODEL.get_dimension()\n",
        "def compute_embedding_matrix(embedding_model, idx_to_word):\n",
        "  \"\"\"\n",
        "  Return the embedding matrix of the train vocabulary:\n",
        "  \"\"\"\n",
        "  embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "  for idx,word in enumerate(idx_to_word):\n",
        "    embedding_matrix[idx] = embedding_model.get_word_vector(word)\n",
        "  return embedding_matrix\n",
        "\n",
        "EMBEDDING_MATRIX = compute_embedding_matrix(EMBEDDING_MODEL, IDX_TO_WORD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L-9Zfst7-3Bf"
      },
      "outputs": [],
      "source": [
        "END_NUMBER=VECTORIZER('[END]')[2]\n",
        "PAD_NUMBER=VECTORIZER('')[-1]\n",
        "START_NUMBER=VECTORIZER('[Start]')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xLZCocd9OKOt",
        "outputId": "4ca1ec49-f1ae-4fe4-a2a6-319b81b7f18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[END]\n",
            "\n",
            "[START]\n"
          ]
        }
      ],
      "source": [
        "WORD_LENGTH[END_NUMBER]=0 #non words are considered of lenght zero\n",
        "WORD_LENGTH[PAD_NUMBER]=0\n",
        "WORD_LENGTH[START_NUMBER]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYzjUi0btOH_"
      },
      "outputs": [],
      "source": [
        "IDX_TO_WORD=tf.constant(IDX_TO_WORD) \n",
        "EMBEDDING_MATRIX =tf.constant(EMBEDDING_MATRIX ) #tensor are easier to manipulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PePDVqxmUu_6"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, initializer, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        \n",
        "\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim, embeddings_initializer=initializer, mask_zero=True\n",
        "        )\n",
        "        self.token_embeddings.trainable=False\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpJcR4e2_-Cs"
      },
      "outputs": [],
      "source": [
        "WORD_LENGTH[1]=sum(WORD_LENGTH[2:])/(len(WORD_LENGTH)-4) #lenght of unknown token is evalueted as the mean of the lenghts of all words in the vocabulary\n",
        "\n",
        "WORD_LENGTH=tf.constant(WORD_LENGTH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlaqFbIuYp2P"
      },
      "outputs": [],
      "source": [
        "class GULPEASE_index(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name='gulpease_index'\n",
        "  def __call__(self,to_cast,y_pred):\n",
        "    \"\"\"\n",
        "    https://it.wikipedia.org/wiki/Indice_Gulpease\n",
        "    the implementation is impossible but if we take the lenght o  all the words in the vocabulary \n",
        "    and multiply by the probability of each word we have a possible representation of the number of \n",
        "    characters per sentence\n",
        "    the number of words can  be inferred as max -number of non words.\n",
        "    \"\"\"\n",
        "    global IDX_TO_WORD,END_NUMBER\n",
        "    #removing all zero length words from the total\n",
        "    n_words=MAX_SEQ_LENGHT-tf.reduce_sum(y_pred[:,:,END_NUMBER],axis=1)-tf.reduce_sum(y_pred[:,:,PAD_NUMBER],axis=1)-tf.reduce_sum(y_pred[:,:,START_NUMBER],axis=1)\n",
        "    tot_char=tf.math.reduce_sum(y_pred*WORD_LENGTH,axis=[1,2])\n",
        "    return tf.reduce_mean((89 +(300 -tot_char*10)/n_words)/100)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Simplification_loss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'simplification_loss'\n",
        "    self.loss = tf.keras.losses.CosineSimilarity()\n",
        "    self.gulpease=GULPEASE_index()\n",
        "  def __call__(self, input,y_pred):\n",
        "    \"\"\"\n",
        "    multiply the embedding matrix for the output this is the \n",
        "    \"\"\"\n",
        "    global EMBEDDING_MATRIX\n",
        "\n",
        "   \n",
        "    # Calculate the loss for each item in the batch.\n",
        "    gulpease=self.gulpease(None,y_pred)\n",
        "    gulpease=10*(0.7-gulpease)*(0.7-gulpease)\n",
        "    input=tf.cast(input,tf.int32)\n",
        "    embd_inp=tf.gather_nd(EMBEDDING_MATRIX,tf.expand_dims(input,-1))\n",
        "    embd_out=tf.reduce_mean(y_pred,axis=1)\n",
        "    embd_out=tf.expand_dims(embd_out,axis=-1)*EMBEDDING_MATRIX\n",
        "    \n",
        "\n",
        "    embd=tf.reduce_mean(embd_inp,axis=1)\n",
        "    #only padding is removed in the count of words the other non words have non zero embedding \n",
        "    n_words=MAX_SEQ_LENGHT-tf.reduce_sum(y_pred[:,:,PAD_NUMBER],axis=1) \n",
        "   \n",
        "    mean_input=embd/tf.expand_dims(tf.reduce_sum(tf.cast(input!= 0,tf.float32),axis=1),axis=-1)\n",
        "\n",
        "    mean_output=tf.reduce_sum(embd_out,axis=1)/tf.expand_dims(n_words,axis=-1)\n",
        "    # Mask off the losses on padding.\n",
        "    \n",
        "    # Return the total.\n",
        "    return tf.cast(1,tf.float32)+self.loss(mean_input,mean_output)-gulpease\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def return_loss():\n",
        "  simp_loss=Simplification_loss()\n",
        "  def custom_loss(y_true, y_pred):\n",
        "    weighted_loss = simp_loss(y_true, y_pred) # your implementation of weighted crossentropy loss\n",
        "    #unweighted_loss = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    return weighted_loss#K.in_train_phase(weighted_loss, unweighted_loss)\n",
        "  return custom_loss\n",
        "def return_accuracy():\n",
        "  accuracy=tf.keras.metrics.Accuracy()\n",
        "\n",
        "  def custom_accuracy(y_true, y_pred):\n",
        "    train_acc=0\n",
        "    test_acc=accuracy(y_true, y_pred)\n",
        "    return K.in_train_phase(train_acc,test_acc) \n",
        "  return custom_accuracy\n",
        "\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "  def train_step(self, data):\n",
        "    global VECTORIZER\n",
        "    x=VECTORIZER(data)\n",
        "   \n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred=self([x,x], training=True)\n",
        "        \n",
        "      loss = self.compiled_loss(x,y_pred)\n",
        "   ion layer by passing a list of vocabulary terms to the layer's __init__() meth trainable_vars = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    # Update weights\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "    # Update metrics (includes the metric that tracks the loss)\n",
        "    self.compiled_metrics.update_state(x,y_pred)\n",
        "    return{'loss':loss}#{m.name: m.result() for m in self.metrics}\n",
        "  def test_step(self, data):\n",
        "    # Unpack the data\n",
        "    x,y=data\n",
        "    x=tf.cast(x,tf.int32)\n",
        "    y=tf.cast(y,tf.int32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    # Compute predictions\n",
        "    y_pred = self([x,x], training=False)\n",
        "    # Updates the metrics tracking the loss\n",
        "    loss=cce(y, y_pred)\n",
        "    # Update the metrics.\n",
        "    self.compiled_metrics.update_state(x,y_pred)\n",
        "    # Return a dict mapping metric names to current value.\n",
        "    # Note that it will include the loss (tracked in self.metrics).\n",
        "    return {m.name:m.result() for m in self.metrics}\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpmkDsFxU6k-"
      },
      "outputs": [],
      "source": [
        "def get_model(initializer = \"uniform\", latent_dim = 2048, num_heads = 8):\n",
        "  global MAX_SEQ_LENGHT, VOCAB_SIZE, EMBEDDING_DIM,VECTORIZER\n",
        "  encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "  x = PositionalEmbedding(MAX_SEQ_LENGHT, VOCABULARY_SIZE, EMBEDDING_DIM, initializer)(encoder_inputs)\n",
        "  encoder_outputs = TransformerEncoder(EMBEDDING_DIM, latent_dim, num_heads)(x)\n",
        "  encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "  decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "  encoded_seq_inputs = keras.Input(shape=(None, EMBEDDING_DIM), name=\"decoder_state_inputs\")\n",
        "\n",
        "  x = PositionalEmbedding(MAX_SEQ_LENGHT, VOCABULARY_SIZE, EMBEDDING_DIM, initializer)(decoder_inputs)\n",
        "  x = TransformerDecoder(EMBEDDING_DIM, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  decoder_outputs = layers.Dense(VOCABULARY_SIZE, activation=\"softmax\")(x)\n",
        "  decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "  decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "  return CustomModel(\n",
        "      [encoder_inputs, decoder_inputs], [decoder_outputs,encoder_outputs], name=\"transformer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp32eSe7WRfo"
      },
      "outputs": [],
      "source": [
        "LATENT_DIM = 2048\n",
        "N_HEADS = 8\n",
        "INITIALIZER = keras.initializers.Constant(EMBEDDING_MATRIX)\n",
        "\n",
        "transformer = get_model(INITIALIZER, LATENT_DIM, N_HEADS)\n",
        "\n",
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EujVaDFvCw0G"
      },
      "outputs": [],
      "source": [
        "checkpoint_path='/content/drive/MyDrive/UniBO/NLP/training_2/'\n",
        "EPOCHS = 10\n",
        "tf.config.run_functions_eagerly(True)\n",
        "callbacks = [\n",
        "      EarlyStopping(patience=3, verbose=1),\n",
        "      ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.00001, verbose=1),\n",
        "      ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "  ]\n",
        "transformer.compile(Adam(learning_rate = 1e-4), loss=return_loss(), metrics=[GULPEASE_index(),Simplification_loss()])\n",
        "\n",
        "history = transformer.fit(TRAIN_DATASET, \n",
        "                            epochs=EPOCHS, \n",
        "                            validation_data=TEST_DATASET,\n",
        "                            callbacks=callbacks\n",
        "                            )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copia_di_Self_Supervised_transformer.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}